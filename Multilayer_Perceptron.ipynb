{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please note: This is a coding challenge I set for myself.*\n",
    "\n",
    "*Please note: Idea of this challenge is based on the book \"Make Your Own Neural Network\" by Tariq Rashid*\n",
    "\n",
    "## Coding Challenge:\n",
    "1. Get more practice with jupyter notebook\n",
    "2. Get more practice with git\n",
    "3. Get more practice with python\n",
    "4. Write a simple multilayer perceptron and show that it is able to learn the MNIST handwritten digit database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "The MNIST database is a large database of handwritten digits(0-9). The dataset is commonly used for training various image processing systems. To train a multilayer perceptron(MLP) on pictures we need to convert the image matrix to a vector. For the MNIST dataset, this means we need to convert a 28x28 matrix into a vector with the size of 784. The general layout of the network will be:\n",
    "\n",
    "* input layer with 784 input nodes (for the 28x28 images)\n",
    "* one hidden layer with x nodes\n",
    "* output layer with 10 nodes (for the labels 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(object):\n",
    "    \"\"\"A simple multilayer perceptron\n",
    "\n",
    "    :param input_nodes: An int, the number of input nodes\n",
    "    :param input_nodes: An int, the number of hidden nodes\n",
    "    :param output_nodes: An int, the number of output nodes\n",
    "    :param weights: A numpy array of the shape(3,1), the weight matrix\n",
    "    :param activation_function: activation function at the output node -> in this case a step function\n",
    "    :param learning_rate : A float, the learning speed coefficient\n",
    "    :param error : A numpy array, holding the errors for the current epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "\n",
    "        self.w_i_h = np.random.normal(0.0, pow(input_nodes, -0.5), (hidden_nodes, input_nodes))\n",
    "        self.w_h_o = np.random.normal(0.0, pow(hidden_nodes, -0.5), (output_nodes, hidden_nodes))\n",
    "\n",
    "        self.output_error = 1\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: 1 / ( 1 + np.exp(-x) )\n",
    "        self.activation_function_derivative = lambda x: x * ( 1 - x )\n",
    "        self.activation_function_inverse = lambda x: special.logit(x)\n",
    "    \n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\" Performance a single training steep. Run the feedforward step, \n",
    "            run backpropagation and update the weights\"\"\"\n",
    "        hidden_outputs, final_outputs = self.feedforward(inputs)\n",
    "        self.update_weights(inputs, targets, hidden_outputs, final_outputs)\n",
    "\n",
    "    def update_weights(self, inputs, targets, hidden_outputs, final_outputs):\n",
    "        \"\"\"Calculate the output error, perform backpropagation and update the weights\"\"\"\n",
    "        # calculate errors\n",
    "        self.output_error = targets - final_outputs\n",
    "        hidden_errors = np.dot(self.w_h_o.T, self.output_error)     \n",
    "        # update weights \n",
    "        self.w_h_o += self.lr * np.dot((self.output_error * final_outputs * (1.0 - final_outputs)), hidden_outputs.T)\n",
    "        self.w_i_h += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), inputs.T)\n",
    "        \n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\" Performance a single feedforward steep through the whole network.\"\"\"\n",
    "        hidden_inputs = np.dot(self.w_i_h, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.w_h_o, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return hidden_outputs, final_outputs\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Performance a single feedforward steep through the whole network \n",
    "            and get the resulting network prediction\"\"\"\n",
    "        _, final_outputs = self.feedforward(inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "    \n",
    "    def rescale(self, unscaled_values):\n",
    "        \"\"\" Helper function to scale the input to a value between 0.01 and 0.99 \"\"\"\n",
    "        scaled_values = unscaled_values\n",
    "        scaled_values -= np.min(scaled_values)\n",
    "        scaled_values /= np.max(scaled_values)\n",
    "        scaled_values *= 0.99\n",
    "        scaled_values += 0.01\n",
    "        \n",
    "        return scaled_values\n",
    "        \n",
    "    def backquery(self, targets):\n",
    "        \"\"\" Run the network backward, given a label, too see the learned generalization \"\"\"\n",
    "        \n",
    "        final_inputs = self.activation_function_inverse(targets)\n",
    "        # calculate the signal out of the hidden layer\n",
    "        hidden_outputs = np.dot(self.w_h_o.T, final_inputs)\n",
    "        # scale them back to 0.01 to .99\n",
    "        hidden_outputs = self.rescale(hidden_outputs)\n",
    "        \n",
    "        # calculate the signal into the hidden layer\n",
    "        hidden_inputs = self.activation_function_inverse(hidden_outputs)\n",
    "        # calculate the signal out of the input layer\n",
    "        inputs = np.dot(self.w_i_h.T, hidden_inputs)\n",
    "        # scale them back to 0.01 to .99\n",
    "        inputs = self.rescale(inputs)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multilayer_perceptron = MultilayerPerceptron(input_nodes = 784, \n",
    "                                             hidden_nodes = 200, \n",
    "                                             output_nodes = 10, \n",
    "                                             learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    with open(\"mnist_dataset/mnist_train.csv\", 'r') as mnist_train:\n",
    "        for record in mnist_train.readlines():\n",
    "            all_values = record.split(',')\n",
    "            # scale and shift the inputs\n",
    "            inputs = (np.asfarray(all_values[1:]).T / 255.0 * 0.99) + 0.01\n",
    "            # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "            targets = np.zeros(10).T + 0.01\n",
    "            # all_values[0] is the target label for this record\n",
    "            targets[int(all_values[0])] = 0.99\n",
    "            multilayer_perceptron.train(np.array(inputs, ndmin=2).T, np.array(targets, ndmin=2).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall performance =  0.9725\n"
     ]
    }
   ],
   "source": [
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for e in range(epochs):\n",
    "    with open(\"mnist_dataset/mnist_test.csv\", 'r') as mnist_test:\n",
    "        for record in mnist_test.readlines():\n",
    "            all_values = record.split(',')\n",
    "            # correct answer is first value\n",
    "            correct_label = int(all_values[0])\n",
    "            # scale and shift the inputs\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "            outputs = multilayer_perceptron.predict(np.array(inputs, ndmin=2).T)\n",
    "            # the index of the highest value corresponds to the label\n",
    "            label = np.argmax(outputs)\n",
    "\n",
    "            if (label == correct_label):\n",
    "                scorecard.append(1)\n",
    "            else:\n",
    "                scorecard.append(0)\n",
    "                \n",
    "# calculate the accuracy of the network \n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"Overall performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backquery\n",
    "\n",
    "Backquery means to run the network backward, given a label, too see the learned generalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18f2e589be0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSdT/ApA6d5WqLQofAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18f2eae0828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# label to test\n",
    "label = 0\n",
    "# create the output signals for this label\n",
    "output_nodes = 10\n",
    "targets = np.zeros(output_nodes) + 0.01\n",
    "# all_values[0] is the target label for this record\n",
    "targets[label] = 0.99\n",
    "print(targets)\n",
    "\n",
    "image_data = multilayer_perceptron.backquery(np.array(targets, ndmin=2).T)\n",
    "plt.imshow(image_data.reshape(28,28), cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
